import Foundation
import SwiftUI
import Combine
import WhisperKit
import AVFoundation
import CoreMedia
import SwiftData

// MARK: - MemoryMonitorService Stub
// Temporary implementation until the full service can be included in the build

class MemoryMonitorService: ObservableObject {
    static let shared = MemoryMonitorService()
    
    enum MemoryPressureLevel: String, CaseIterable {
        case normal = "normal"
        case warning = "warning"
        case critical = "critical"
    }
    
    @Published var currentMemoryUsage: UInt64 = 0
    @Published var memoryPressureLevel: MemoryPressureLevel = .normal
    
    private init() {}
    
    func startRecordingMonitoring() {}
    func stopRecordingMonitoring() {}
    func performMemoryCleanup() {}
    func startIntensiveMonitoring() {}
}

/// æ–‡å­—èµ·ã“ã—ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿
struct TranscriptionSegment: Codable, Identifiable {
    let id: UUID
    let startTime: Double      // é–‹å§‹æ™‚é–“ï¼ˆç§’ï¼‰
    let endTime: Double        // çµ‚äº†æ™‚é–“ï¼ˆç§’ï¼‰
    let text: String          // ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
    let confidence: Float?    // ä¿¡é ¼åº¦ï¼ˆå°†æ¥æ‹¡å¼µç”¨ï¼‰
    
    init(startTime: Double, endTime: Double, text: String, confidence: Float? = nil, id: UUID = UUID()) {
        self.id = id
        self.startTime = startTime
        self.endTime = endTime
        self.text = text
        self.confidence = confidence
    }
}

/// WhisperKit ãƒ¢ãƒ‡ãƒ«é¸æŠåˆ—æŒ™å‹ï¼ˆä¸Šä½3ã¤ã®ã¿ï¼‰
enum WhisperKitModel: String, CaseIterable, Identifiable {
    var id: String { rawValue }
    
    // ä¸Šä½3ã¤ã®æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ï¼ˆå¤§â†’ä¸­â†’å°ã®é †ï¼‰
    case large = "large-v3"
    case medium = "medium"
    case small = "small"
    
    var displayName: String {
        switch self {
        case .medium: return "ãƒãƒ©ãƒ³ã‚¹ (1GB) - é«˜å“è³ª"
        case .small: return "æ¨™æº– (500MB) - æ¨å¥¨"
        case .large: return "é«˜ç²¾åº¦ (1.5GB) - æœ€é«˜å“è³ª"
        }
    }
    
    var description: String {
        switch self {
        case .medium: return "é«˜å“è³ªãªéŸ³å£°èªè­˜ã€ãƒ“ã‚¸ãƒã‚¹åˆ©ç”¨ã«æœ€é©"
        case .small: return "ç²¾åº¦ã¨é€Ÿåº¦ã®æœ€é©ãƒãƒ©ãƒ³ã‚¹ã€æ—¥å¸¸ä½¿ç”¨ã«æœ€é©"
        case .large: return "æœ€é«˜ç²¾åº¦ã€å°‚é–€ç”¨é€”ãƒ»é‡è¦ãªä¼šè­°å‘ã‘"
        }
    }
    
    var isRecommended: Bool {
        return true // å…¨ã¦ã®ãƒ¢ãƒ‡ãƒ«ãŒæ¨å¥¨ï¼ˆä¸Šä½3ã¤ã®ã¿ãªã®ã§ï¼‰
    }
    
    var estimatedSize: String {
        switch self {
        case .medium: return "1GB"
        case .small: return "500MB"
        case .large: return "1.5GB"
        }
    }
    
    static var recommendedModels: [WhisperKitModel] {
        return allCases // å…¨ã¦ãŒæ¨å¥¨ãƒ¢ãƒ‡ãƒ«
    }
}

class WhisperKitTranscriptionService: ObservableObject {
    // MARK: - Published Properties
    
    @Published var isTranscribing: Bool = false
    @Published var errorMessage: String?
    @Published var transcriptionText: String = ""
    @Published var transcriptionProgress: Double = 0.0
    @Published var transcriptionStage: String = ""
    @Published var selectedModel: WhisperKitModel = .small
    @Published var transcriptionLanguage = TranscriptionLanguage.japanese
    @Published var downloadedModels: Set<WhisperKitModel> = []
    @Published var downloadingModels: Set<WhisperKitModel> = []
    @Published var downloadErrorModels: Set<WhisperKitModel> = []
    @Published var downloadProgress: [WhisperKitModel: Double] = [:]
    @Published var isInitialized: Bool = false
    
    // MARK: - Transcription Results
    
    var lastTranscriptionTimestamps: String?
    var lastTranscriptionSegments: [TranscriptionSegment]?
    
    // MARK: - WhisperKit Instance
    
    private var whisperKit: WhisperKit?
    
    static let shared = WhisperKitTranscriptionService()
    
    private init() {
        // Start with uninitialized state - will initialize on first use
    }
    
    // MARK: - Public Methods
    
    func transcribeAudioFile(at audioURL: URL) async throws {
        print("ğŸ—£ï¸ Starting WhisperKit transcription for file: \(audioURL.lastPathComponent)")
        
        await MainActor.run {
            isTranscribing = true
            transcriptionText = ""
            transcriptionProgress = 0.0
            transcriptionStage = "åˆæœŸåŒ–ä¸­..."
            errorMessage = nil
            
            // Clear previous transcription results
            lastTranscriptionTimestamps = nil
            lastTranscriptionSegments = nil
        }
        
        let startTime = Date()
        
        // Initialize WhisperKit if needed or force reinitialize for fresh state
        if !isInitialized || whisperKit == nil {
            await MainActor.run {
                transcriptionStage = "ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­..."
                transcriptionProgress = 0.1
            }
            
            do {
                // Force a fresh initialization to clear any cached state
                await MainActor.run {
                    whisperKit = nil
                    isInitialized = false
                }
                try await initializeWhisperKit()
            } catch {
                await MainActor.run {
                    errorMessage = "WhisperKit initialization failed: \(error.localizedDescription)"
                    isTranscribing = false
                }
                throw error
            }
        }
        
        await MainActor.run {
            transcriptionStage = "éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­..."
            transcriptionProgress = 0.3
        }
        
        // Transcribe using WhisperKit
        do {
            guard let whisperKit = whisperKit else {
                throw NSError(domain: "WhisperKitTranscriptionService", code: 1, userInfo: [
                    NSLocalizedDescriptionKey: "WhisperKit instance not available after initialization"
                ])
            }
            
            await MainActor.run {
                transcriptionStage = "éŸ³å£°èªè­˜å®Ÿè¡Œä¸­..."
                transcriptionProgress = 0.7
            }
            
            // Get audio file info for quality check
            let audioFileInfo = try await getAudioFileInfo(url: audioURL)
            print("ğŸµ Audio file info: duration=\(audioFileInfo.duration)s, format=\(audioFileInfo.format)")
            
            // Audio quality check
            if audioFileInfo.duration < 1.0 {
                print("âš ï¸ Audio file too short (\(audioFileInfo.duration)s), may cause music detection")
            }
            
            // Check audio level to prevent music misdetection
            let audioLevel = try await getAudioLevel(url: audioURL)
            print("ğŸ”Š Audio file adjusted level: \(String(format: "%.4f", audioLevel))")
            
            // ã‚ˆã‚Šæ­£ç¢ºãªéŸ³å£°ãƒ¬ãƒ™ãƒ«åˆ¤å®šé–¾å€¤
            let lowLevelThreshold: Float = 0.005  // ã‚ˆã‚Šä½ã„é–¾å€¤ã§åˆ¤å®š
            let veryLowLevelThreshold: Float = 0.002  // éå¸¸ã«ä½ã„ãƒ¬ãƒ™ãƒ«
            
            if audioLevel < veryLowLevelThreshold {
                print("âŒ Extremely low audio level detected (\(String(format: "%.4f", audioLevel))), high risk of music detection")
                print("ğŸ’¡ Recommendation: Record much closer to microphone or increase input gain significantly")
            } else if audioLevel < lowLevelThreshold {
                print("âš ï¸ Low audio level detected (\(String(format: "%.4f", audioLevel))), may cause music detection")
                print("ğŸ’¡ Tip: Record closer to microphone or increase input gain")
            } else {
                print("âœ… Audio level is adequate (\(String(format: "%.4f", audioLevel))), good for transcription")
            }
            
            // Enhanced Japanese speech recognition settings (proven configuration)
            print("ğŸ¯ Starting WhisperKit transcription with enhanced Japanese speech recognition")
            print("ğŸ”§ Model: \(selectedModel.rawValue), Language: ja, Temperature: 0.0")
            
            // åˆæœŸå‹•ä½œå®Ÿç¸¾ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®è¨­å®šï¼ˆã‚·ãƒ³ãƒ—ãƒ«ã§ç¢ºå®Ÿï¼‰
            let decodingOptions = DecodingOptions(
                verbose: true,
                task: .transcribe,
                language: "ja",  // æ—¥æœ¬èª
                temperature: 0.0,
                temperatureIncrementOnFallback: 0.2,
                temperatureFallbackCount: 5,
                sampleLength: 224,
                usePrefillPrompt: true,
                usePrefillCache: true,
                skipSpecialTokens: true,
                withoutTimestamps: false  // ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æœ‰åŠ¹åŒ–
            )
            
            print("ğŸ—£ï¸ Using language: ja (æ—¥æœ¬èª)")
            print("ğŸ”§ Audio file: \(audioURL.lastPathComponent)")
            print("ğŸ”§ WhisperKit initialized: \(isInitialized)")
            
            let transcriptionResults = try await whisperKit.transcribe(
                audioPath: audioURL.path, 
                decodeOptions: decodingOptions
            )
            
            // Process transcription results - WhisperKit returns an array
            let resultText = await MainActor.run { () -> String in
                if !transcriptionResults.isEmpty {
                    // Use the text from the first result
                    let mainText = transcriptionResults.first?.text ?? ""
                    
                    // Enhanced post-processing for music detection errors
                    let cleanedText = postProcessTranscriptionResult(mainText)
                    
                    // Format text with segment breaks
                    if cleanedText.isEmpty {
                        print("ğŸ”§ Main text is empty after cleaning, trying to merge segments...")
                        let allSegmentTexts = transcriptionResults.flatMap { $0.segments }.map { 
                            postProcessTranscriptionResult($0.text.trimmingCharacters(in: CharacterSet.whitespacesAndNewlines))
                        }.filter { !$0.isEmpty }
                        let mergedText = allSegmentTexts.joined(separator: "\n")
                        print("ğŸ”§ Merged segment text with line breaks: '\(mergedText)'")
                        return mergedText
                    } else {
                        // If main text exists, add line breaks by segment
                        if let firstResult = transcriptionResults.first, !firstResult.segments.isEmpty {
                            let segmentTexts = firstResult.segments.map { 
                                postProcessTranscriptionResult($0.text.trimmingCharacters(in: CharacterSet.whitespacesAndNewlines))
                            }.filter { !$0.isEmpty }
                            
                            if !segmentTexts.isEmpty {
                                let formattedText = segmentTexts.joined(separator: "\n")
                                print("ğŸ”§ Formatted text with segment breaks: '\(formattedText)'")
                                return formattedText
                            } else {
                                return cleanedText
                            }
                        } else {
                            return cleanedText
                        }
                    }
                } else {
                    print("âŒ No transcription results returned")
                    return ""
                }
            }
            
            // Store segments if available  
            await MainActor.run {
                if let firstResult = transcriptionResults.first, !firstResult.segments.isEmpty {
                    lastTranscriptionSegments = firstResult.segments.map { segment in
                        TranscriptionSegment(
                            startTime: Double(segment.start),
                            endTime: Double(segment.end),
                            text: segment.text
                        )
                    }
                }
            }
            
            // Retry logic for empty or invalid results
            let finalResultText: String
            if (resultText.isEmpty || isOnlySpecialTokens(resultText)) && audioFileInfo.duration > 2.0 {
                print("ğŸ”„ Empty or invalid result detected, attempting retry with speech-focused settings...")
                print("ğŸ”„ Original result: '\(resultText)'")
                finalResultText = try await retryWithSpeechFocusedSettings(audioURL: audioURL, whisperKit: whisperKit)
            } else {
                finalResultText = resultText
            }
            
            await MainActor.run {
                transcriptionText = finalResultText
                transcriptionProgress = 1.0
                transcriptionStage = "å®Œäº†"
                isTranscribing = false
                
                let duration = Date().timeIntervalSince(startTime)
                print("âœ… Transcription completed in \(String(format: "%.1f", duration))s")
                print("ğŸ“ Final result: '\(transcriptionText)'")
            }
            
        } catch {
            await MainActor.run {
                errorMessage = "Transcription failed: \(error.localizedDescription)"
                isTranscribing = false
            }
            print("âŒ Transcription failed: \(error)")
            throw error
        }
    }
    
    func changeModel(to model: WhisperKitModel) async {
        await MainActor.run {
            selectedModel = model
            downloadedModels.insert(model)
        }
    }
    
    func retryTranscription(audioURL: URL) async throws {
        // Retry the transcription with the same logic as transcribeAudioFile
        try await transcribeAudioFile(at: audioURL)
    }
    
    // MARK: - Utility Methods
    
    func segmentsFromJSON(_ jsonString: String) -> [TranscriptionSegment] {
        guard let data = jsonString.data(using: .utf8) else { return [] }
        do {
            return try JSONDecoder().decode([TranscriptionSegment].self, from: data)
        } catch {
            print("âŒ Failed to decode segments from JSON: \(error)")
            return []
        }
    }
    
    func segmentsToJSON(_ segments: [TranscriptionSegment]) -> String {
        do {
            let data = try JSONEncoder().encode(segments)
            return String(data: data, encoding: .utf8) ?? ""
        } catch {
            print("âŒ Failed to encode segments to JSON: \(error)")
            return ""
        }
    }
    
    func formatTimestamp(_ timeInterval: Double) -> String {
        let minutes = Int(timeInterval) / 60
        let seconds = Int(timeInterval) % 60
        let milliseconds = Int((timeInterval.truncatingRemainder(dividingBy: 1)) * 1000)
        return String(format: "%02d:%02d.%03d", minutes, seconds, milliseconds)
    }
    
    /// WhisperKitã®åˆæœŸåŒ–
    @MainActor
    private func initializeWhisperKit() async throws {
        print("ğŸ”§ Initializing WhisperKit with model: \(selectedModel.rawValue)")
        
        transcriptionStage = "WhisperKitåˆæœŸåŒ–ä¸­..."
        transcriptionProgress = 0.1
        
        do {
            // WhisperKitè¨­å®š - ã‚·ãƒ³ãƒ—ãƒ«ã§ç¢ºå®Ÿãªè¨­å®šï¼ˆå‹•ä½œå®Ÿç¸¾ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰
            let config = WhisperKitConfig(model: selectedModel.rawValue)
            
            // WhisperKitã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ã¦ä¿å­˜
            whisperKit = try await WhisperKit(config)
            print("âœ… WhisperKit initialized successfully with model: \(selectedModel.rawValue)")
            
            isInitialized = true
            transcriptionStage = "åˆæœŸåŒ–å®Œäº†"
            transcriptionProgress = 0.2
            
            // ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯
            downloadedModels.insert(selectedModel)
            downloadingModels.remove(selectedModel)
            downloadErrorModels.remove(selectedModel)
            
        } catch {
            print("âŒ WhisperKit initialization failed: \(error)")
            
            isInitialized = false
            downloadErrorModels.insert(selectedModel)
            downloadingModels.remove(selectedModel)
            
            throw error
        }
    }
    
    /// æ–‡å­—èµ·ã“ã—çµæœã®å¾Œå‡¦ç†ï¼ˆéŸ³æ¥½åˆ¤å®šã‚¨ãƒ©ãƒ¼ã¨ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®ä¿®æ­£ï¼‰
    private func postProcessTranscriptionResult(_ text: String) -> String {
        let trimmedText = text.trimmingCharacters(in: CharacterSet.whitespacesAndNewlines)
        
        // ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å»
        var cleanedText = removeSpecialTokens(from: trimmedText)
        
        // éŸ³æ¥½ã¨ã—ã¦èª¤åˆ¤å®šã•ã‚ŒãŸå ´åˆã®å‡¦ç†ã¨ç¿»è¨³ã‚¨ãƒ©ãƒ¼ã®æ¤œå‡º
        let musicPatterns = [
            "(éŸ³æ¥½)", "[éŸ³æ¥½]", "â™ª", "â™«", "â™ªâ™«", "(Music)", "[Music]",
            "(BGM)", "[BGM]", "(èƒŒæ™¯éŸ³æ¥½)", "[èƒŒæ™¯éŸ³æ¥½]",
            "(speaking in foreign language)", "(foreign language)",
            "(speaking in a foreign language)", "speaking in foreign language",
            "(The sound of a gunshot)", "(Laughing)", "(Congratulations!)",
            "(Thank you for watching)", "The train is now in the middle",
            "The train will leave from Charlotte", "Washington DC"
        ]
        
        // éŸ³æ¥½ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ã®å ´åˆã¯ç©ºæ–‡å­—ã‚’è¿”ã™
        for pattern in musicPatterns {
            if cleanedText == pattern {
                print("ğŸµ Music pattern detected: '\(pattern)' - treating as empty")
                return ""
            }
        }
        
        // éŸ³æ¥½ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å«ã‚€å ´åˆã¯é™¤å»
        for pattern in musicPatterns {
            cleanedText = cleanedText.replacingOccurrences(of: pattern, with: "")
        }
        
        // å‰å¾Œã®ç©ºç™½ã‚’å†åº¦é™¤å»
        cleanedText = cleanedText.trimmingCharacters(in: CharacterSet.whitespacesAndNewlines)
        
        // ç©ºã®çµæœã®å ´åˆã¯ãƒ­ã‚°å‡ºåŠ›
        if cleanedText.isEmpty && !trimmedText.isEmpty {
            print("ğŸ”§ Text cleaned to empty: original='\(trimmedText)' -> cleaned='\(cleanedText)'")
        }
        
        return cleanedText
    }
    
    /// ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å»
    private func removeSpecialTokens(from text: String) -> String {
        var cleanedText = text
        
        // WhisperKitç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆç¿»è¨³é–¢é€£ã‚’å¼·åŒ–ï¼‰
        let specialTokenPatterns = [
            "<\\|startoftranscript\\|>",
            "<\\|endoftext\\|>",
            "<\\|ja\\|>",                // æ—¥æœ¬èªã‚¿ã‚°
            "<\\|en\\|>",                // è‹±èªã‚¿ã‚°  
            "<\\|zh\\|>",                // ä¸­å›½èªã‚¿ã‚°
            "<\\|ko\\|>",                // éŸ“å›½èªã‚¿ã‚°
            "<\\|ru\\|>",                // ãƒ­ã‚·ã‚¢èªã‚¿ã‚°
            "<\\|fr\\|>",                // ãƒ•ãƒ©ãƒ³ã‚¹èªã‚¿ã‚°
            "<\\|de\\|>",                // ãƒ‰ã‚¤ãƒ„èªã‚¿ã‚°
            "<\\|es\\|>",                // ã‚¹ãƒšã‚¤ãƒ³èªã‚¿ã‚°
            "<\\|[a-z]{2}\\|>",          // ãã®ä»–ã®è¨€èªã‚¿ã‚°ï¼ˆ2æ–‡å­—ï¼‰
            "<\\|transcribe\\|>",        // è»¢å†™ã‚¿ã‚°
            "<\\|translate\\|>",         // ç¿»è¨³ã‚¿ã‚°ï¼ˆé‡è¦ï¼‰
            "<\\|\\d+\\.\\d+\\|>",       // ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ— <|0.00|>
            "<\\|notimestamps\\|>",
            "<\\|nospeech\\|>",
            "<\\|music\\|>",
            "<\\|silence\\|>",
            "<\\|[^>]+\\|>"              // ãã®ä»–ã®ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆåŒ…æ‹¬çš„ï¼‰
        ]
        
        // æ­£è¦è¡¨ç¾ã§ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å»
        for pattern in specialTokenPatterns {
            cleanedText = cleanedText.replacingOccurrences(
                of: pattern,
                with: "",
                options: .regularExpression
            )
        }
        
        // é€£ç¶šã™ã‚‹ç©ºç™½ã‚’å˜ä¸€ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›
        cleanedText = cleanedText.replacingOccurrences(
            of: "\\s+",
            with: " ",
            options: .regularExpression
        )
        
        return cleanedText.trimmingCharacters(in: CharacterSet.whitespacesAndNewlines)
    }
    
    /// ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã‹ã©ã†ã‹ã‚’åˆ¤å®š
    private func isOnlySpecialTokens(_ text: String) -> Bool {
        let cleanedText = removeSpecialTokens(from: text)
        return cleanedText.isEmpty
    }
    
    /// éŸ³å£°ç‰¹åŒ–è¨­å®šã§ã®å†è©¦è¡Œ
    private func retryWithSpeechFocusedSettings(audioURL: URL, whisperKit: WhisperKit) async throws -> String {
        print("ğŸ¯ Retrying with speech-focused settings to avoid music detection")
        
        let retryTranscription = try await whisperKit.transcribe(
            audioPath: audioURL.path,
            decodeOptions: DecodingOptions(
                verbose: true,
                task: .transcribe,
                language: "ja",
                temperature: 0.3,  // å°‘ã—é«˜ã„æ¸©åº¦ã§å¤šæ§˜æ€§ç¢ºä¿
                temperatureIncrementOnFallback: 0.2,
                temperatureFallbackCount: 3,
                sampleLength: 224,
                usePrefillPrompt: true,
                usePrefillCache: false,  // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–ã§æ–°é®®ãªçµæœ
                skipSpecialTokens: true,
                withoutTimestamps: true  // ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãªã—ã§ã‚¯ãƒªãƒ¼ãƒ³
            )
        )
        
        // å†è©¦è¡Œçµæœã®å‡¦ç†
        if !retryTranscription.isEmpty {
            let retryText = retryTranscription.first?.text ?? ""
            let cleanedRetryText = postProcessTranscriptionResult(retryText)
            
            print("ğŸ”„ Retry result: '\(cleanedRetryText)'")
            return cleanedRetryText
        }
        
        print("ğŸ”„ Retry also returned empty result")
        return ""
    }
    
    /// éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°æƒ…å ±ã‚’å–å¾—ï¼ˆå‹•ä½œå®Ÿç¸¾ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰
    private func getAudioFileInfo(url: URL) async throws -> (duration: TimeInterval, format: String) {
        let asset = AVURLAsset(url: url)
        
        // ãƒ•ã‚¡ã‚¤ãƒ«ã®åŸºæœ¬æƒ…å ±ã‚’éåŒæœŸã§å–å¾—
        let duration = try await asset.load(.duration)
        let durationSeconds = CMTimeGetSeconds(duration)
        
        // ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼æƒ…å ±ã‚’å–å¾—ï¼ˆå¤ã„APIä½¿ç”¨ï¼‰
        let tracks = try await asset.loadTracks(withMediaType: .audio)
        let timeScale = try await tracks.first?.load(.naturalTimeScale) ?? 44100
        let format = timeScale.description
        
        return (duration: durationSeconds, format: format)
    }
    
    /// éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®éŸ³é‡ãƒ¬ãƒ™ãƒ«ã‚’æ¸¬å®šï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    private func getAudioLevel(url: URL) async throws -> Float {
        let asset = AVURLAsset(url: url)
        let tracks = try await asset.loadTracks(withMediaType: .audio)
        
        guard let audioTrack = tracks.first else {
            return 0.0
        }
        
        // AVAssetReaderã§éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚Š
        let assetReader = try AVAssetReader(asset: asset)
        let readerOutput = AVAssetReaderTrackOutput(track: audioTrack, outputSettings: [
            AVFormatIDKey: kAudioFormatLinearPCM,
            AVLinearPCMBitDepthKey: 16,
            AVLinearPCMIsBigEndianKey: false,
            AVLinearPCMIsFloatKey: false,
            AVLinearPCMIsNonInterleaved: false
        ])
        
        assetReader.add(readerOutput)
        assetReader.startReading()
        
        var maxLevel: Float = 0.0
        var rmsSum: Double = 0.0
        var totalSamples = 0
        var activeSamples = 0 // ç„¡éŸ³ã§ãªã„ã‚µãƒ³ãƒ—ãƒ«æ•°
        let silenceThreshold: Float = 0.001 // ç„¡éŸ³åˆ¤å®šé–¾å€¤
        
        // ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã‚’åˆ†æï¼ˆæœ€å¤§5ç§’ã¾ã§ï¼‰
        let maxAnalysisDuration = 5.0 // ç§’
        let maxSamples = Int(16000 * maxAnalysisDuration)
        
        while assetReader.status == .reading && totalSamples < maxSamples {
            guard let sampleBuffer = readerOutput.copyNextSampleBuffer() else { break }
            
            if let blockBuffer = CMSampleBufferGetDataBuffer(sampleBuffer) {
                let length = CMBlockBufferGetDataLength(blockBuffer)
                var data = Data(count: length)
                
                data.withUnsafeMutableBytes { bytes in
                    CMBlockBufferCopyDataBytes(blockBuffer, atOffset: 0, dataLength: length, destination: bytes.baseAddress!)
                }
                
                // 16-bit PCMãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦è§£æ
                let samples = data.withUnsafeBytes { bytes in
                    return bytes.bindMemory(to: Int16.self)
                }
                
                for sample in samples {
                    let normalizedSample = abs(Float(sample)) / Float(Int16.max)
                    
                    // ãƒ”ãƒ¼ã‚¯å€¤æ›´æ–°
                    maxLevel = max(maxLevel, normalizedSample)
                    
                    // RMSè¨ˆç®—ç”¨
                    rmsSum += Double(normalizedSample * normalizedSample)
                    totalSamples += 1
                    
                    // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ï¼ˆç„¡éŸ³ã§ãªã„ï¼‰ã®ã‚«ã‚¦ãƒ³ãƒˆ
                    if normalizedSample > silenceThreshold {
                        activeSamples += 1
                    }
                }
            }
        }
        
        assetReader.cancelReading()
        
        // éŸ³å£°å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—
        let rmsLevel = totalSamples > 0 ? Float(sqrt(rmsSum / Double(totalSamples))) : 0.0
        let activityRatio = totalSamples > 0 ? Float(activeSamples) / Float(totalSamples) : 0.0
        
        // ã‚ˆã‚Šæ­£ç¢ºãªéŸ³å£°ãƒ¬ãƒ™ãƒ«åˆ¤å®š
        // RMSãƒ¬ãƒ™ãƒ«ã¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ç‡ã‚’çµ„ã¿åˆã‚ã›ã¦ç·åˆè©•ä¾¡
        let adjustedLevel = rmsLevel * (0.3 + 0.7 * activityRatio) // ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã«å¿œã˜ã¦é‡ã¿èª¿æ•´
        
        print("ğŸ”Š Audio analysis details:")
        print("   - Peak level: \(String(format: "%.4f", maxLevel))")
        print("   - RMS level: \(String(format: "%.4f", rmsLevel))")
        print("   - Activity ratio: \(String(format: "%.1f", activityRatio * 100))%")
        print("   - Adjusted level: \(String(format: "%.4f", adjustedLevel))")
        print("   - Total samples analyzed: \(totalSamples)")
        
        return adjustedLevel
    }
    
    
    @MainActor
    func reinitialize() async {
        isInitialized = false
        whisperKit = nil
        do {
            try await initializeWhisperKit()
        } catch {
            print("âŒ Reinitialize failed: \(error)")
        }
    }
}